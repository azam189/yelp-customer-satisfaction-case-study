#This script extracts necessary data from different parts of the Yelp dataset in different formats and compresses it into 2 major files/datasets:
1. 'output_1' is the main dataframe consisting of businesses and their attributes expanded by check-in information. Only Businesses in Missouri are kept as this is the state with the highest variance in weather.
2. 'yelp_academic_dataset_review_reduced' holds review data for all relevant busi-nesses from the main dataframe and helps to reduce file size significantly (20x).



{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7edd07-ba64-4e5c-b787-d66eff012db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "def load_and_process_business_data(input_path):\n",
    "# Flatten JSON format\n",
    "df_business = pd.read_csv(input_path)\n",
    "df_business['j'] = df_business['j'].apply(json.loads)\n",
    "flattened_data = json_normalize(df_business['j'])\n",
    "df_business = pd.concat([df_business.drop('j', axis=1), flattened_data], axis=1)\n",
    "# Filter and keep only businesses from Missouri to keep the dataset small\n",
    "df_business = df_business[df_business['state'] == 'MO']\n",
    "# Drop empty columns\n",
    "empty_cols_business = [col for col in df_business.columns if df_busi-ness[col].isnull().all()]\n",
    "df_business.drop(empty_cols_business, axis=1, inplace=True)\n",
    "return df_business\n",
    "def load_and_filter_reviews(input_path, start_date, end_date):\n",
    "df_reviews = pd.read_json(input_path, lines=True)\n",
    "df_reviews['date'] = pd.to_datetime(df_reviews['date'])\n",
    "return df_reviews\n",
    "def create_review_table(df_reviews, business_ids):\n",
    "filtered_chunks = [chunk[chunk['business_id'].isin(business_ids)] for chunk in np.ar-ray_split(df_reviews, 24)]\n",
    "return pd.concat(filtered_chunks)\n",
    "def get_checkin_date(date_str, position):\n",
    "if isinstance(date_str, str):\n",
    "dates = date_str.split(',')\n",
    "date = dates[0 if position == 'first' else -1].strip()\n",
    "return datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "return datetime.min\n",
    "def main():\n",
    "start_time = time.time()\n",
    "# Set working directory to where python file is located\n",
    "wd = os.path.dirname(os.path.abspath(__file__))\n",
    "os.chdir(wd)\n",
    "# Set paths and timeframe\n",
    "input_path_business = 'business_data.csv'\n",
    "input_path_checkin = 'yelp_academic_dataset_checkin.json'\n",
    "input_path_reviews = 'yelp_academic_dataset_review.json'\n",
    "timeframe_start = datetime.strptime('2018-01-01', '%Y-%m-%d')\n",
    "timeframe_end = datetime.strptime('2019-12-31', '%Y-%m-%d')\n",
    "# Load data\n",
    "df_business = load_and_process_business_data(input_path_business)\n",
    "df_checkin = pd.read_json(input_path_checkin, lines=True)\n",
    "df_reviews = load_and_filter_reviews(input_path_reviews, timeframe_start, timeframe_end)\n",
    "# Merge dataframes\n",
    "df = pd.merge(df_business, df_checkin, on='business_id', how='left')\n",
    "# Process review data\n",
    "if not os.path.exists('yelp_academic_dataset_review_reduced.csv'):\n",
    "business_id_set = set(df['business_id'].unique())\n",
    "df_reviews_reduced = create_review_table(df_reviews, business_id_set)\n",
    "df_reviews_reduced.to_csv('yelp_academic_dataset_review_reduced.csv', in-dex=False)\n",
    "# Filter data based on check-in dates\n",
    "df['last_checkin'] = df['date'].apply(lambda x: get_checkin_date(x, 'last'))\n",
    "df['first_checkin'] = df['date'].apply(lambda x: get_checkin_date(x, 'first'))\n",
    "df = df[(df['last_checkin'] >= timeframe_start) & (df['first_checkin'] <= timeframe_end)]\n",
    "# Remove duplicates\n",
    "duplicates = df['business_id'].duplicated().sum()\n",
    "print(f\"Number of duplicates in 'business_id': {duplicates}\")\n",
    "if duplicates > 0:\n",
    "df.drop_duplicates(subset='business_id', inplace=True)\n",
    "# Export Data\n",
    "with pd.ExcelWriter('output_1.xlsx') as writer:\n",
    "df.to_excel(writer, sheet_name='sheet_1', index=False)\n",
    "df.to_csv('output_1.csv', index=False)\n",
    "print('--- Runtime: %s seconds ---' % (time.time() - start_time))\n",
    "if __name__ == \"__main__\":\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
